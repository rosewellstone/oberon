\section{Analog vs. Digital: Binary/Decimal}
In the 1960s, the computing world consisted of two camps: the analog and the
digital world. It was by no means clear to which the future would belong. Typically,
mathematicians belonged to the digital, electrical engineers to the analog camps.
Digital computers were exact, but required very many expensive components,
whereas engineers were used to live with approximate results, correct to perhaps 3
or 4 decimal digits. In the analog world, only addition and integration (over time) is
easily achieved, whereas multiplication and division are difficult or almost
impossible. Today, the then heated controversy over analog vs. digital has
completely vanished. This is not only due to the enormous reduction in price of
digital circuitry, but primarily because analog values were very hard, if not
impossible, to store. Computers are now not so much used to compute, but rather
to store data. As a consequence, analog computers have died out.

Remains to be noted that digital is actually an illusion. The physical world is analog
except deep down on the atomic level of quantum physics. Even stored values are
often not digital (not binary). Modern memories (2016) consist of cells capable of
distinguishing between 8 (analog) values.

As an aside, the adjective digital stems from the Latin word digis (digitis) meaning
finger. This suggests that digital computers count by holding up fingers like first
year pupils. The adjective analog reflects the fact that a computed value is
represented by an analogous quantity, typically a voltage. It would be much more
sensible to distinguish between discrete and continuous, rather than digital and
analog.

Within the digital community, there existed a further schism. It divided the world
into binary and decimal camps. Binary computers - which are the rule today -
represent integers as binary numbers, i.e. as a sequence of binary digits (bits), the
one at position $n$ with weight 2". Decimal computers represent them as sequences
of decimal digits, the one at position $n$ with weight 10", each decimal digit being
encoded by 4 bits. Large companies offered both kinds of computers at
considerable expense, although evidently the binary representation is definitely
more economical. The reason behind this separation lay in the financial world's
insistence that computers produce in every case exactly the same results as
calculation by hand would, even if erroneous. Such errors may happen in the case
of division.

The dilemma was "solved" in 1964 by IBM's System 360 by offering both number
representations by a large instruction set.
