\chapter{Storage Layout and Management}
\label{ch:MM}
Centralized RM (CRM) is a crucial property of Oberon.
\begin{description}
  \item[Its advantage] is that replication of management algorithms and a premature partitioning
    of resources are avoided.
  \item[The disadvantage] is that management algorithms are fixed once and forever and remain
    the same for all applications.
\end{description}
The success of a CRM therefore depends crucially on its flexibility and its efficient implementation.
This chapter presents the scheme and the algorithms governing main storage in Oberon.

\section{Layout and run-time organization}
The storage layout of Oberon is determined by the structure of code and data typical in the
use of modular, high-level programming languages, and in particular of the language Oberon.
It suggests the subdivision of storage into 3 areas:
\begin{description}
  \item[module space] Each module specifies procedures (code) and global (static) variables.
    Its initialization can be regarded as a procedure implicitly called upon loading. Space
    must be allocated for code and data in \emph{blocks} by the loader. Typically, modules
    contain very few global variables, hence the space size is primarily determined by code.
  \item[workspace (stack)] Execution of every command invokes a sequence of procedures, each
    of which uses (possibly zero) parameters and local variables. Since procedure calls and
    completions follow a strict first-in last-out order, the stack is the uniquely suited
    strategy for local storage allocation. Deallocation upon completion of a procedure is
    achieved by merely resetting the pointer identifying the top of the stack. Since this
    operation is performed by a single instruction, it costs virtually no time. Because
    Oberon is a single-process system, a single stack suffices. Furthermore, after completion
    of a command, the stack is empty. This fact will be important in simplifying the scheme
    for reclamation of dynamically allocated space.
  \item[dynamic space (heap)] Apart from global (static) variables, and local (stack-allocated)
    variables, a program may refer to anonymous variables referenced through pointers. Such
    variables are allocated truly dynamically through calls of an explicit \verb|NEW| operation.
    These variables are allocated in the so-called \emph{heap}. Their deallocation is "automatic",
    when free storage is needed and they are no longer referenced from any of the loaded modules.
    This process is called \emph{garbage collection} (GC). Every record allocated in the heap
    contains a (hidden) pointer to the type descriptor, called \emph{type tag}, used by GC.
\end{description}
Unfortunately, the number of distinct spaces is larger than 2. If it were 2, no arbitrary
size limitation would be necessary; merely the sum of their sizes would be inherently limited
by the size of the store. In the case of 3 spaces, arbitrarily determined size limits are
unavoidable. Address mapping hardware can alleviate (and delegate) this problem using a
virtual address space which is so large that limits will hardly ever be reached.

Such a scheme is implemented by tables mapping virtual into physical addresses, requiring
multiple memory accesses for every reference. Of course, the need for a double or a triple
access for every memory reference is avoided by a translation cache in the (hardware) unit.
Nevertheless, a decrease in performance is unavoidable for each cache miss. Furthermore,
an additional subcycle is required for every access in order to look up the cached translation
table. Without a virtual address scheme, each module block must consist of an integral number
of physically adjacent pages. Holes generated by the release of modules must be reused. We
employ the simple scheme of marking the released space as a hole, and of allocating a new
block in the 1st hole encountered that is large enough (1st-fit strategy). Considering the
relative infrequency of module releases, efforts to improve the strategy are not worth the
resulting added complexity.

It is remarkable that the code for module allocation and release without virtual addressing
is only marginally more complicated than with it. The only remaining advantages of an MMU are a
better storage utilization, because no holes occur (a negligible advantage), and that inadvertent
references to unloaded modules, e.g. via installed procedures, lead to an invalid address trap.

It is worth recalling that the concept of address mapping was introduced as a requirement for
virtual memory implemented with disks as backing store, where pages could be moved into the
background in order to obtain space for newly required pages, and could then be retrieved from
disk on demand, i.e. when access was requested. This scheme is called \emph{demand paging}.
It is not used in Oberon, and one may fairly state that demand paging has lost its significance
with the availability of large, primary stores.

Experience in the use of the RISC predecessor Ceres leads to the conclusion that whereas
address translation through an MMU was an essential feature for multi-user OSes, it constitutes
a dispensible overkill for single-user workstations. The fact that modern semiconductor technology
made it possible to integrate the entire translation and caching scheme into a single chip,
or even into the processor itself, led to the hiding (and ignoring) of the scheme's considerable
complexity. Its side effects on execution speed are essentially unpredictable. This makes systems
with MMU virtually unusable for applications with tight real-time constraints. The RISC processor
does indeed not feature an address mapping unit.
\begin{figure}[h!]
  \label{fig:storage-layout}
  \centering
  \includegraphics[width=.6\textwidth]{i/q}
  \caption{Storage layout}
\end{figure}

The RISC processor features 16 registers (of 32 bits). R0 - R11 are for expression evaluation.
R12 - R15 have fixed, system-wide usage:
\begin{table}[h!]
  \centering
  \begin{tabular}{l l}
    R12 & address of the module table MT (typically constant) \\
    R13 & base address for variables in the current module SB \\
        & (static base) \\
    R14 & stack pointer SP \\
    R15 & return address LNK (fixed by RISC's BL instruction)
  \end{tabular}
\end{table}

The used memory layout is shown in Figure \ref{fig:storage-layout}.

\section{Heap Management}
The term \emph{dynamic storage} is used here for all variables that are allocated neither
statically (global variables) nor on the stack (local variables), but through invocation
of the intrinsic procedure \verb|NEW| in the heap. Such variables are anonymous and are
referenced exclusively via pointers.

The space allocated to such dynamic variables becomes free and reusable as soon as the last
reference to it vanishes. This event is hard, and in multiprocess systems even impossible
to detect. The usual remedy is to ignore it and instead to determine the accessibility of
all allocated variables (records, objects) only at the time when more storage space is needed.
This process is then called \emph{garbage collection}.

Oberon does not provide an explicit deallocation procedure allowing the programmer to signal
that a variable will no longer be referenced. The 1st reason for this omission is that
usually a programmer would not know when to call for deallocation. And 2ndly, this "hint"
could not be taken as trustworthy. An erroneous deallocation, i.e. one occurring when there
still exist references to the object in question, could lead to a multiple allocation of
the same space with disastrous consequences. Hence, it appears wise to fully rely on system
management to determine which areas of the store are truly reusable.

Before discussing the scheme for storage reclamation, which is the primary subject of
heap management, we turn our attention to the problem of allocation, i.e. the implementation
of procedure \verb|NEW|. The simplest solution is to maintain a list of free blocks and to
pick the 1st one large enough. This strategy leads to a relatively large fragmentation of
space and produces many small elements, particularly in the 1st part of the list. We
therefore employ a somewhat more refined scheme and maintain 4 lists of available space.
3 of them contain pieces of fixed size, namely 32, 64, and 128 bytes. The 4th list contains
pieces whose size is any multiple of 256. We note that the choice of the values permits the
merging of any 2 contiguous elements into an element of the next list. This scheme keeps
fragmentation, i.e. the emergence of small pieces in large numbers, reasonably low with
minimal effort. The body of procedure \verb|NEW| consists of relatively few instructions,
and typically only a small fraction of them needs to be executed.

The statement \verb|NEW(p)| is compiled into an instruction sequence assigning the address
of pointer variable $p$ to a fixed register (\verb|R0|) and the type tag to another register
(\verb|R1|). The type tag is a pointer to a type descriptor containing information required
by GC. This includes the size of the space occupied and now to be allocated. The effect of
\verb|NEW| is the assignment of the address of the allocated block to $p$, and the assignment
of the tag to a prefix of the block. (see Fig. \ref{fig:allocation})
\begin{figure}[h!]
  \label{fig:allocation}
  \centering
  \includegraphics[width=\textwidth]{i/r}
  \caption{Heap allocation of dynamic variable $p$\^{} by NEW($p$)}
\end{figure}

In conclusion, we emphasize that this scheme makes the allocation of an object very efficient.
Nevertheless, it is considerably more costly than that of a variable explicitly declared
and therefore allocated either globally or on the stack. We now turn to the problem of
storage reclamation or GC. There exist 2 essentially different schemes:
\begin{itemize}
  \item[$1^{st}$,] the reference counting, and
  \item[$2^{nd}$,] the mark-scan schemes.
\end{itemize}
In the former, every object carries a (hidden) \emph{reference count}, indicating the number
of existing references to it. The scheme works as follows:
\begin{enumerate}
  \item \verb|NEW(p)| initializes the reference count of $p$\^{} to 1.
  \item An assignment \verb|q := p| decrements the reference count of $q$\^{} by 1, performs
    the assignment, then increments the reference count of $p$\^{} by 1.
\end{enumerate}
When a reference count reaches zero, the element is linked into the free list. There are 2
disadvantages inherent in this approach:
\begin{enumerate}
  \item the non-negligible overhead in pointer assignments.
  \item circular data structures never become recognized as free,
    even if no external references point to their elements.
\end{enumerate}

Oberon employs the 2nd scheme which involves no hidden operations like the 1st one, but
relies on a process initiated when free storage has become scarce and more is needed.
It consists of 2 phases:
\begin{description}
  \item[mark phase] all referenced and therefore still accessible elements are marked.
  \item[scan phase] their unmarked complement is released.
\end{description}
Its primary disadvantage is that the process may be started at moments unpredictable to the
system's user. During the process, the computer then appears to be blocked. It follows that
an interactive system using mark-scan GC must guarantee that the process is sufficiently
fast in order to be hardly noticeable. Modern processors make this possible, even with
large main stores. Nevertheless, finding all accessible nodes in an entire computer system
within, say, a second appears to be a formidable feat.

We recognize that the mark phase essentially is a tree traversal, or rather a forest traversal.
The roots of the trees are all named pointer variables in existence. We shall postpone the
question of how these roots are to be found, and 1st present a quick tutorial about tree
traversal. In general, nodes of the traversed structure may contain many pointers (branches).
We shall, however, 1st restrict our attention to a binary tree, because the essential problem
and its solution can be explained better in this way.

The essential problem alluded to is that of storage utilization by the traversal algorithm itself.
Typically, information about the nodes already visited must be retained, be it explicitly, or
implicitly as in the case of use of recursion. Such a strategy is plainly unacceptable,
because the amount of storage needed is unpredictable and may become very large, and because
GC is typically initiated just when more storage is unavailable. The task may seem impossible,
yet a solution lies in the idea of inverting pointers along the path traversed, thus keeping
the return path open. It is embodied in the following procedure, whose task is to traverse
the tree given by the parameter root, and to mark every node. Mark values are assumed to
be initially 0. Let the data structure be defined by the types
\begin{verbatim}
  Ptr = POINTER TO Node;
  Node = RECORD m: INT; L, R: Ptr END;
\end{verbatim}
and the algorithm by the procedure
\begin{verbatim}
  PROC traverse(root: Ptr);
    VAR p, q, r: Ptr;
  BEGIN p := root; q := root;
    REPEAT (*p # NIL*) INC(p.m); (*mark*)
      IF p.L # NIL THEN (*pointer rotation*)
        r := p.L; p.L := p.R; p.R := q; q := p; p := r
      ELSE
        p.L := p.R; p.R := q; q := NIL
      END
    UNTIL p = q
  END traverse
\end{verbatim}
\begin{figure}[h!]
  \centering
  \includegraphics[width=.25\textwidth]{i/s}
  \label{fig:rotation}
  \caption{Rotation of four pointers}
\end{figure}
We note that only 3 local variables are required, independent of the size of the tree to be
traversed. The 3rd, $r$, is in fact merely an auxiliary variable to perform the rotation of
values $p.L$, $p.R$, $q$, and $p$ as shown in Fig. \ref{fig:rotation}. A snapshot of a tree
traversal is shown in Fig. \ref{fig:tree-traversal}.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{i/t}
  \label{fig:tree-traversal}
  \caption{Tree traversal (original at left, snapshot at right)}
\end{figure}

The pair $p$, $q$ of pointers marks the position of the process. The algorithm traverses
the tree in a left to right, depth 1st fashion. When it returns to the root, all nodes
have been marked. How are these claims convincingly supported? The best way is by analyzing
the algorithm at an arbitrary node. We start with the hypothesis H that, given the initial
state P, the algorithm will reach state Q, (see Fig \ref{fig:transition}). State Q differs
from P by the node and its descendants B and C having been marked, and by an exchange of p
and q. We now apply the algorithm to state P, assuming that B and C are not empty. The process
is illustrated in Fig \ref{fig:transition}. P0 stands for P in Fig. \ref{fig:tree-traversal}.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{i/u}
  \label{fig:transition}
  \caption{Transition from state P to Q}
\end{figure}
Transitions $P0 \rightarrow P1$, $P2 \rightarrow P3$, and $P4 \rightarrow P5$ are the direct
results of applying the pointer rotation as specified by the sequence of five assignments
in the algorithm. Transitions $P1 \rightarrow P2$ and $P3 \rightarrow P4$ follow from the
hypothesis H being applied to the states P1 and P3: subtrees are marked and p, q interchanged.
We note in passing that the node is visited 3 times. Progress is recorded by the mark value
which is incremented from 0 to 3.

Fig. \ref{fig:transition-3-times}. demonstrates that, if H holds for steps $P1 \rightarrow P2$
and $P3 \rightarrow P4$, then it also holds for step $P1 \rightarrow P5$, which visits the
subtree p. Hence, it also holds for the step root $\rightarrow$ root, which traverses the entire tree.
\begin{figure}[h!]
  \label{fig:transition-3-times}
  \centering
  \includegraphics[width=.9\textwidth]{i/v}
  \caption{Transitions from P0 to P5, visiting nodes 3 times}
\end{figure}

This proof by recursion relies on the algorithm performing correct transitions also in the
case of p.L being NIL, i.e. B being the empty tree. In this case, state P1 is skipped; the
1st transition is $P0 \rightarrow P2$ (see Fig \ref{fig:transition-direct}).
If p.L is again NIL, i.e. also C is empty, the next transition is $P2 \rightarrow P4$.
This concludes the demonstration of the algorithm's correctness.
\begin{figure}[h!]
  \label{fig:transition-direct}
  \centering
  \includegraphics[width=\textwidth]{i/w}
  \caption{Direct transition from P0 to P2, if $p.L = NIL$}
\end{figure}

We now modify the algorithm of tree traversal to the case where the structure is not confined
to a binary tree, but may be a tree of any degree, i.e. each node may have any number $n$ of
descendants. For practical purposes, however, we restrict $n$ to be in the range $0 \leq n
\leq N$, and therefore may represent all nodes by the type
\begin{verbatim}
  Node = RECORD m, n: INT;
           dsc: ARRAY N OF Node
         END
\end{verbatim}
In principle, the binary tree traversal algorithm might be adopted almost without change,
merely extending the rotation of pointers from p.L, p.R, q, p to p.dsc[0], ... , p.dsc[n-1], q, p. However, this
would be an unnecessarily inefficient solution. The following is a more effective variant. Moreover, it
caters for the case of inhomogeneous graphs, where different nodes have different numbers of
descendants. The key lies in associating with every node, in addition to the tag, a 2nd private
field mk. It serves 2 purposes. The 1st is as a mark, with mk > 0 indicating that the node had
been visited. The 2nd is to store the address of the next descendant to be visited. The
underlying data structure is shown in Fig \ref{fig:record}. Type descriptors consist of the following fields:
\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{2pt}
  \begin{tabular}{l l}
    size & {\small in bytes, of the described record type,} \\
    base & {\small a table of pointers to the base types descriptors(3 elements only)} \\
    {\small offsets} & {\small of the descendant pointers in the described type(1 word each)}
  \end{tabular}
\end{table}
\begin{figure}[h!]
  \label{fig:record}
  \centering
  \includegraphics[width=.9\textwidth]{i/x}
  \caption{Record and its type descriptor}
\end{figure}

We note that the mark value, starting with zero (unmarked), is used as a counter of descendants
already traversed, and hence as an index to the descendant field to be processed next. The
algorithm can be applied not only to trees, but to arbitrary structures, including circular ones, if the
continuation condition p \# 0 (actually p >= heapOrg) is extended to (p >= heapOrg) \& (offadr = 0).
This causes a descendant that is already marked to be skipped. Here the array M stands for the
entire memory.
\begin{verbatim}
  PROC traverse(root: Ptr);
    VAR offadr, offset: INT; p, q, r: Ptr;
  BEGIN p := root; q := root;
    REPEAT (*p # NIL*) offadr := p.mk; (*mark*)
      IF offadr = 0 THEN
        tag := p.tg; offadr := tag + 16
      ELSE INC(offadr, 4) END;
      p.mk := offadr; offset := M[offadr];
      IF offset # -1 THEN (*move down*)
        r := M[p+offset]; offadr := M[r-4];
        IF offadr = 0 THEN
          M[p+offset] := q; q := p; p := r
        END
      ELSE (*move up*)
        offadr := M[q-4]; offset := M[offadr]
        IF p # q THEN
          r := M[q+offset]; M[q+offset] := p;
          p := q; q := r
        END
      END
    UNTIL (p = q) & (offset = -1)
  END traverse.
\end{verbatim}

The mark is included in each record's hidden prefix. The prefix takes 2 words only; the first is used
for the tag. The other is reserved for the garbage collector and used as mark and offset address.
The end of the list of descendant pointers is marked by an entry with value -1. And finally,
assignments involving M are expressed as
\begin{table}[h!]
  \centering
  \begin{tabular}{c|l}
    assignment & for \\\hline
    \verb|SYSTEM.GET(a, x)| & $x := M[a]$ \\
    \verb|SYSTEM.PUT(a, x)| & $M[a] := x$ \\
  \end{tabular}
\end{table}

The scan phase is performed by a relatively straight-forward algorithm. The heap, i.e. the storage
area between HeapOrg and HeapLimit (the latter is a variable), is scanned element by element,
starting at HeapOrg. Elements marked are unmarked, and unmarked elements are freed by linking
them into the appropriate list of available space.
As the heap may always contain free elements, the scan phase must be able to recognize them in
order to skip them or merge them with an adjacent free element. For this purpose, the free
elements are also considered as prefixed. The prefix serves to determine the element's size and to
recognize it as free due to a special (negative) mark value. The encountered mark values and the
action to be taken are:
\begin{table}[h!]
  \centering
  \begin{tabular}{c r c}
    $mk$ value &  state & action \\\hline
    $=0$     & unmarked & collect, mark free \\
    $>0$     &   marked & unmark \\
    $<0$     &     free & skip or merge
  \end{tabular}
\end{table}

\section{Kernel}
\label{sec:kernel}
The kernel lies at the bottom of the module hierarchy. It contains the procedures for
dynamic storage allocation and retrieval as described before. The procedures are \verb|New|,
\verb|Mark|, and \verb|Scan|. \verb|Kernel| also contains the driver routines for the disk.
They are used by modules \verb|FileDir| and \verb|Files|. The "disk" is actually an SD-card,
a high-volume flash-RAM. It is accessed purely sequentially, byte-wise, by a standard,
serial peripheral interface (SPI). Within \verb|Kernel| a table called \verb|SectorMap| is
allocated keeping track of blocks (sectors) occupied by files. A single bit indicates,
whether a sector is allocated or not. This table is accessed by the procedures
\verb|AllocSector|, \verb|MarkSector|, and \verb|FreeSector|. Reading and writing is done
sector-wise by procedures \verb|GetSector| and \verb|PutSector|. Sector numbers are always
a multiple of 29 for the purpose of redundancy checks.

Furthermore, the kernel contains a timer counting milliseconds and, perhaps, a real time
clock, showing date and time. Clock data are packed into a single word as follows:
\begin{figure}[h!]
  \label{fig:datetime}
  \centering
  \includegraphics[width=\textwidth]{i/y}
  \caption{Encoding of date and time (year starting with 2000)}
\end{figure}
\begin{verbatim}
  DEFINITION Kernel;(*NW/PR 11.4.86/27.12.95/15.5.2013*)
    CONST SectorLength = 1024;
    TYPE Sector = ARRAY SectorLength OF BYTE;
    VAR allocated, NofSectors, heapOrg, heapLim,
        stackOrg, MemSize: INT;
    PROC New(VAR ptr: INT; tag: INT);
    PROC Mark(pref: INT);
    PROC Scan;
    PROC ResetDisk;
    PROC MarkSector(sec: INT);
    PROC FreeSector(sec: INT);
    PROC AllocSector(hint: INT; VAR sec: INT);
    PROC GetSector(src: INT; VAR dest: Sector);
    PROC PutSector(dest: INT; VAR src: Sector);
    PROC Time(): INT; (*milliseconds*)
    PROC Clock(): INT;
    PROC SetClock(dt: INT);
    PROC Install(adr, procadr: INT);
    PROC Init;
  END Kernel.
\end{verbatim}

\section{The storage management toolbox}
The user can obtain information about the system state and resources through its toolbox,
a set of commands contained in the too module \verb|System|. These commands are:
\begin{verbatim}
  PROC Watch;
  PROC Collect;  / n
  PROC SetClock; / year,month,day,hour,minute,second
\end{verbatim}
Command \verb|Watch| shows the amount of storage occupied in the heap, the number of disk
sectors allocated on the disk, and the number of tasks installed. The command \verb|Collect|
allows to control the frequency of GC. The number $n$ indicates how many commands are
executed before the next GC.
